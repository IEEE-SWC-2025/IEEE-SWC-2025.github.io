---
layout: default
title: Roadmap of Federated Learning - from Motivation to Practice
---

<!-- Hero Section -->
<section
  class="jumbotron text-center"
  style="
    background-image: url('/assets/images/shubham-dhage-T9rKvI3N0NM-unsplash.jpg');
    background-size: cover;
    background-position: 50% 40%;
    min-height: 60vh;
    position: relative;
    color: white;
  "
>
  <div class="hero-section" style="min-height: 60vh">
    <div
      class="page-section container px-5 pt-5 d-flex flex-column align-items-center gap-3"
    >
      <h1 class="display-5 fw-bolder mt-5">
        Roadmap of Federated Learning: from Motivation to Practice
      </h1>
      <h3>
        Presented by:
        <br />
        Dr. Jiayu Zhou , University of Michigan <br />
        Dr. Steve Drew, University of Calgary <br />Guojun Tang, University of
        Calgary
      </h3>
    </div>
  </div>
</section>

<section
  class="page-section container-fluid px-5 pt-5 pb-5 mt-2 d-flex flex-column gap-4"
>
  <div class="d-flex flex-column gap-4">
    <div class="row">
      <div class="col-12 mb-3 mb-sm-0 d-flex flex-column gap-4">
        <h3>
          Join us to explore how federated learning enables private,
          collaborative model training across distributed data
        </h3>
        <p class="fs-5">
          Federated learning (FL), unlike traditional centralized learning
          methods, provides a distributed machine learning (ML) paradigm that
          enables different clients to train a global model collaboratively
          without directly uploading or sharing their private datasets. Due to
          its feasibility, FL has achieved outstanding success in a wide range
          of applications, from edge computing and the Internet of Things (IoT)
          to healthcare and finance.
          <br /><br />
          In this tutorial, we will present a comprehensive roadmap of federated
          learning, including (1) the motivation of FL, (2) current challenges
          of FL, (3) practical applications of FL, and (4) hands-on experiments
          on FL. By this tutorial, the intended audience may learn about the
          history of federated learning and the cutting-edge algorithms in FL.
        </p>
        <h4>Tutorial Outline</h4>
        <p class="fs-5">
          We first dive into FL based on a real-world scenario: how we train a
          keyboard input prediction model among tremendous usersâ€™ devices. In
          traditional centralized machine learning methods, we require users to
          upload their data to participate in the training, which will violate
          the data privacy and cause significant communication overhead. To
          address the aforementioned issue, Google proposed a pioneering
          decentralized machine learning framework entitled Federated Learning
          that only requires users to upload the model parameters instead of raw
          data to participate in the model training. We will introduce the
          details of FedAvg, the most typical FL algorithm, and its efficacy in
          this scenario. In the rest of this section, we will talk about other
          concepts of FL, such as the taxonomy of FL and the current popular FL
          software frameworks.
        </p>
        <div>
          <p class="fs-5">
            In the second section, we will discuss the current challenges in FL
            and their corresponding solutions as follows:
          </p>
          <ol type="1" class="fs-5">
            <li>
              Data heterogeneity: Clients hold data drawn from different
              distributions. For example, some specific classes of samples
              concentrate on few clients, while other clients only hold very
              limited data of them. Data heterogeneity can often slow down or
              even prevent model convergence. The typical mitigations of this
              issue include using a regularization term [5], knowledge
              distillation [12], model contrastive loss [3], and control-variate
              correction [4].
            </li>
            <li>
              System heterogeneity: Clients in FL usually differ in computing
              resources, power, and network quality. It leads to the drop-off
              and even bias against weaker devices in the system. We may
              alleviate it by setting up a proper system design [6] or
              empowering the weaker devices to train a thinner sub-layer of the
              model [11].
            </li>
            <li>
              Efficiency: FL is a distributed system, which requires considering
              the communication efficiency. There are some cutting-edge
              algorithms to cut off the communication overhead by using the
              prototype training [7] and one-shot FL [15] so that clients and
              servers only interact with each other in one round of
              communication.
            </li>
            <li>
              There is still a probability that the malicious user may obtain
              the original data from the model uploaded by clients [8]. The
              typical methods used to enhance security include homomorphic
              encryption [9] and secure aggregation [10].
            </li>
          </ol>
        </div>
        <p class="fs-5">
          In the last section, there are some practical applications of FL. In
          the first case study we will present how FL improves the productivity
          of edge computing. In the following case, we will talk about a
          real-world application of FL in finance, how to utilize federate
          learning to assist the finance crime detection. The last case will
          demonstrate the FL application in healthcare scenarios.
        </p>
        <div>
          <h4>Timeline</h4>
          <ol type="1" class="fs-5">
            <li>
              Introduction (60 mins)
              <ol type="a">
                <li>Motivation of FL</li>
                <li>Introduction of FedAvg [1]</li>
                <li>
                  Taxonomy of FL (PFL vs global model, Cross-devices vs
                  Cross-silos, and VFL and HFL)
                </li>
                <li>Popular FL software frameworks</li>
              </ol>
            </li>
            <li>
              Challenges (60 mins)
              <ol type="a">
                <li>Data heterogeneity [2,3,4,5,12]</li>
                <li>System heterogeneity [6, 7, 11]</li>
                <li>Efficiency [7, 15]</li>
                <li>Privacy [8,9,10]</li>
              </ol>
            </li>
            <li>
              Practical study cases (30 mins)
              <ol type="a">
                <li>IoT [13]</li>
                <li>Finance [14]</li>
                <li>Healthcare (cross-silos with multi-modal case)</li>
              </ol>
            </li>
          </ol>
        </div>
        <div>
          <h4>References</h4>
          <p class="fw-light">
            [1] McMahan B, Moore E, Ramage D, et al. Communication-efficient
            learning of deep networks from decentralized data[C]//Artificial
            intelligence and statistics. PMLR, 2017: 1273-1282.
            <br />
            [2] Zhao Y, Li M, Lai L, et al. Federated learning with non-iid
            data[J]. arXiv preprint arXiv:1806.00582, 2018.
            <br />
            [3] Li Q, He B, Song D. Model-contrastive federated
            learning[C]//Proceedings of the IEEE/CVF conference on computer
            vision and pattern recognition. 2021: 10713-10722.
            <br />
            [4] Karimireddy S P, Kale S, Mohri M, et al. Scaffold: Stochastic
            controlled averaging for federated learning[C]//International
            conference on machine learning. PMLR, 2020: 5132-5143.
            <br />
            [5] Li T, Sahu A K, Zaheer M, et al. Federated optimization in
            heterogeneous networks[J]. Proceedings of Machine learning and
            systems, 2020, 2: 429-450.
            <br />
            [6] Bonawitz K, Eichner H, Grieskamp W, et al. Towards federated
            learning at scale: System design[J]. Proceedings of machine learning
            and systems, 2019, 1: 374-388.
            <br />
            [7] Tan Y, Long G, Liu L, et al. Fedproto: Federated prototype
            learning across heterogeneous clients[C]//Proceedings of the AAAI
            conference on artificial intelligence. 2022, 36(8): 8432-8440.
            <br />
            [8] Zhu L, Liu Z, Han S. Deep leakage from gradients[J]. Advances in
            neural information processing systems, 2019, 32.
            <br />
            [9] Zhang C, Li S, Xia J, et al. {BatchCrypt}: Efficient homomorphic
            encryption for {Cross-Silo} federated learning[C]//2020 USENIX
            annual technical conference (USENIX ATC 20). 2020: 493-506.
            <br />
            [10] Bonawitz K, Ivanov V, Kreuter B, et al. Practical secure
            aggregation for privacy-preserving machine learning[C]//proceedings
            of the 2017 ACM SIGSAC Conference on Computer and Communications
            Security. 2017: 1175-1191.
            <br />
            [11] Diao E, Ding J, Tarokh V. Heterofl: Computation and
            communication efficient federated learning for heterogeneous
            clients[J]. arXiv preprint arXiv:2010.01264, 2020.
            <br />
            [12] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for
            heterogeneous federated learning[C]//International conference on
            machine learning. PMLR, 2021: 12878-12889.
            <br />
            [13] Wu J, Dong F, Leung H, et al. Topology-aware federated learning
            in edge computing: A comprehensive survey[J]. ACM Computing Surveys,
            2024, 56(10): 1-41.
            <br />
            [14] Zhang H, Hong J, Dong F, et al. A privacy-preserving hybrid
            federated learning framework for financial crime detection[J]. arXiv
            preprint arXiv:2302.03654, 2023. <br />
            [15] Jhunjhunwala D, Wang S, Joshi G. Fedfisher: Leveraging fisher
            information for one-shot federated learning[C]//International
            Conference on Artificial Intelligence and Statistics. PMLR, 2024:
            1612-1620.
          </p>
        </div>
      </div>
    </div>
    <hr />
    <h3>Presenters</h3>
    <div class="row">
      <div
        class="col-md-3 col-sm-4 mb-3 mb-sm-0 d-flex flex-row gap-4 justify-content-center"
      >
        <div class="text-center">
          <img
            src="/assets/images/JZhou-MSU-web2.jpg"
            alt="Dr. Jiayu Zhou"
            class="circle-img mb-3"
          />
          <h5>Dr. Jiayu Zhou</h5>
          <h6><i>University of Michigan, United States</i></h6>
        </div>
      </div>
      <div class="col-md-9 col-sm-8">
        <h4>Biography</h4>
        <p>
          Jiayu Zhou is an Associate Professor in the School of Information at
          the University of Michigan. Before joining Michigan, he was a
          professor of Computer Science at Michigan State University. He
          received his Ph.D. in Computer Science from Arizona State University
          in 2014. Jiayuâ€™s research spans large-scale machine learning, data
          mining, and biomedical informatics, addressing both foundational
          methods and their impactful applications. He has served as a technical
          program committee member for leading international conferences,
          including NeurIPS, ICML, and SIGKDD, and as an Associate Editor for
          journals including ACM Transactions on Computing for Healthcare, ACM
          SIGKDD Explorations, Neurocomputing (Elsevier), and the Journal of
          Alzheimerâ€™s Disease (JAD). His research is generously supported by
          prestigious institutions, including the National Science Foundation
          (NSF), the National Institutes of Health (NIH), and the Office of
          Naval Research (ONR). He is notably recognized with the NSF CAREER
          Award (2018).
          <br /><br />
          Jiayuâ€™s work has received notable recognitions, including the Best
          Student Paper Award at the IEEE International Conference on Data
          Mining (ICDM 2014) and the International Symposium on Biomedical
          Imaging (ISBI 2016), as well as the Best Paper Award at the IEEE
          International Conference on Big Data (BigData 2016). Additionally, his
          team received the Best Paper Award in the Health Track at the 2022
          SIGKDD Conference on Knowledge Discovery and Data Mining. Most
          recently, Jiayu was recognized as one of the winners of the NSF/NIST
          Privacy-Enhancing Technologies Challenge, showcasing his innovations
          in privacy-preserving machine learning at the Summit for Democracy,
          demonstrating a commitment to reinforcing democratic values.
        </p>
      </div>
    </div>
    <div class="row">
      <div
        class="col-md-3 col-sm-4 mb-3 mb-sm-0 d-flex flex-row gap-4 justify-content-center"
      >
        <div class="text-center">
          <img
            src="/assets/images/steve-drew-small.jpg"
            alt="Dr. Steve Drew"
            class="circle-img mb-3"
          />
          <h5>Dr. Steve Drew</h5>
          <h6><i>University of Calgary, Canada</i></h6>
        </div>
      </div>
      <div class="col-md-9 col-sm-8">
        <h4>Biography</h4>
        <p>
          Dr. Steve Drew is an Assistant Professor at the Department of
          Electrical and Software Engineering, University of Calgary. His
          research areas include distributed systems, machine learning,
          cloud/edge computing, and blockchain. He has over ten years of
          experience in the industry. He worked for Cisco Systems on cloud and
          edge service orchestration. He was one of the final winners of the
          NSF/NIST Privacy-Enhancing Technologies Challenges, where his
          privacy-preserving machine learning innovation was showcased at the
          Summit of Democracy as a testament to reinforcing democratic values.
        </p>
      </div>
    </div>
    <div class="row">
      <div
        class="col-md-3 col-sm-4 mb-3 mb-sm-0 d-flex flex-row gap-4 justify-content-center"
      >
        <div class="text-center">
          <img
            src="/assets/images/guojun.jpg"
            alt="Guojun Tang"
            class="circle-img mb-3"
          />
          <h5>Guojun Tang</h5>
          <h6><i>University of Calgary, Canada</i></h6>
        </div>
      </div>
      <div class="col-md-9 col-sm-8">
        <h4>Biography</h4>
        <p>
          Guojun Tang is a PhD student from the University of Calgary under the
          supervision of Dr. Steve Drew. His research interests include
          federated learning, private computing, and data mining.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="container-fluid d-flex flex-column gap-4 bg-light">
  {% include colocated-conferences.html %}
</section>

<section
  class="page-section container-fluid px-5 py-5 d-flex flex-column gap-4"
>
  <h2 class="fs-2 text-center">Sponsors</h2>
  <div class="row justify-content-around gap-3 pb-5">
    <div
      class="col-12 col-md-auto d-flex justify-content-center"
      style="height: 50px"
    >
      <img src="/assets/images/ieee mb black png.png" class="mh-100" />
    </div>
    <div
      class="col-12 col-md-auto d-flex justify-content-center"
      style="height: 50px"
    >
      <img src="/assets/images/cis-logo.png" class="mh-100" />
    </div>
    <div
      class="col-12 col-md-auto d-flex justify-content-center"
      style="height: 50px"
    >
      <img src="/assets/images/IEEE-CS_LogoTM-orange.png" class="mh-100" />
    </div>
  </div>
</section>
